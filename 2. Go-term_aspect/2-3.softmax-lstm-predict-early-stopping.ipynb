{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"raw","source":"import tensorflow as tf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport progressbar\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2023-08-20T15:03:42.809034Z","iopub.execute_input":"2023-08-20T15:03:42.809931Z","iopub.status.idle":"2023-08-20T15:03:42.816477Z","shell.execute_reply.started":"2023-08-20T15:03:42.809883Z","shell.execute_reply":"2023-08-20T15:03:42.815261Z"}}},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"#Load Data \n#Terms for each protein fold\ntrain_terms = pd.read_csv('/kaggle/input/cafa-5-protein-function-prediction/Train/train_terms.tsv',sep='\\t')\n#Embeddings for each aminoacid_sequence\ntrain_embeddings = np.load('/kaggle/input/t5embeds/train_embeds.npy')\n#Protein ID's for the embeddings\ntrain_id = np.load('/kaggle/input/t5embeds/train_ids.npy')","metadata":{"execution":{"iopub.status.busy":"2023-08-20T15:03:42.818974Z","iopub.execute_input":"2023-08-20T15:03:42.819359Z","iopub.status.idle":"2023-08-20T15:03:58.442648Z","shell.execute_reply.started":"2023-08-20T15:03:42.819325Z","shell.execute_reply":"2023-08-20T15:03:58.441495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_id.shape,train_embeddings.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-20T15:03:58.443863Z","iopub.execute_input":"2023-08-20T15:03:58.444267Z","iopub.status.idle":"2023-08-20T15:03:58.45174Z","shell.execute_reply.started":"2023-08-20T15:03:58.444233Z","shell.execute_reply":"2023-08-20T15:03:58.450703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert embeddings numpy array(train_embeddings) into pandas dataframe.\ncolumn_num = train_embeddings.shape[1]\ntrain_df = pd.DataFrame(train_embeddings, columns = [\"Column_\" + str(i) for i in range(1, column_num+1)])\ntrain_df['ID'] = train_id","metadata":{"execution":{"iopub.status.busy":"2023-08-20T15:03:58.454584Z","iopub.execute_input":"2023-08-20T15:03:58.455158Z","iopub.status.idle":"2023-08-20T15:03:58.488611Z","shell.execute_reply.started":"2023-08-20T15:03:58.45511Z","shell.execute_reply":"2023-08-20T15:03:58.487698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Analysis","metadata":{}},{"cell_type":"code","source":"train_terms","metadata":{"execution":{"iopub.status.busy":"2023-08-20T15:03:58.492494Z","iopub.execute_input":"2023-08-20T15:03:58.492793Z","iopub.status.idle":"2023-08-20T15:03:58.512438Z","shell.execute_reply.started":"2023-08-20T15:03:58.49277Z","shell.execute_reply":"2023-08-20T15:03:58.511251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_terms['term'].unique().shape","metadata":{"execution":{"iopub.status.busy":"2023-08-20T15:03:58.513728Z","iopub.execute_input":"2023-08-20T15:03:58.514055Z","iopub.status.idle":"2023-08-20T15:03:59.002529Z","shell.execute_reply.started":"2023-08-20T15:03:58.514021Z","shell.execute_reply":"2023-08-20T15:03:59.001234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df","metadata":{"execution":{"iopub.status.busy":"2023-08-20T15:03:59.004627Z","iopub.execute_input":"2023-08-20T15:03:59.005029Z","iopub.status.idle":"2023-08-20T15:03:59.135929Z","shell.execute_reply.started":"2023-08-20T15:03:59.004995Z","shell.execute_reply":"2023-08-20T15:03:59.134896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Number of different proteins with the same aminoacid sequence embedding \ndf_duplicated = train_df[train_df.loc[:, train_df.columns != 'ID'].duplicated(keep=False)]\n#Flag the pairs \ndf_duplicated['DuplicateGroup'] = df_duplicated.groupby([col for col in df_duplicated.columns]).ngroup()","metadata":{"execution":{"iopub.status.busy":"2023-08-20T15:03:59.137538Z","iopub.execute_input":"2023-08-20T15:03:59.137928Z","iopub.status.idle":"2023-08-20T15:04:25.0601Z","shell.execute_reply.started":"2023-08-20T15:03:59.137893Z","shell.execute_reply":"2023-08-20T15:04:25.058999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_duplicated.shape[0])","metadata":{"execution":{"iopub.status.busy":"2023-08-20T15:04:25.061531Z","iopub.execute_input":"2023-08-20T15:04:25.061968Z","iopub.status.idle":"2023-08-20T15:04:25.068438Z","shell.execute_reply.started":"2023-08-20T15:04:25.061931Z","shell.execute_reply":"2023-08-20T15:04:25.067327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Collect all terms of a EntryID inside one row through a list\ndf_duplicated_terms = train_terms[train_terms['EntryID'].isin(df_duplicated['ID'])].groupby('EntryID')['term'].apply(list).reset_index(name='terms_collected')\ndf_duplicated_terms['duplicated_sequence_group'] = df_duplicated_terms.merge(df_duplicated, left_on='EntryID', right_on='ID')['DuplicateGroup']","metadata":{"execution":{"iopub.status.busy":"2023-08-20T15:04:25.074434Z","iopub.execute_input":"2023-08-20T15:04:25.07487Z","iopub.status.idle":"2023-08-20T15:04:26.171753Z","shell.execute_reply.started":"2023-08-20T15:04:25.074838Z","shell.execute_reply":"2023-08-20T15:04:26.170719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_duplicated_terms","metadata":{"execution":{"iopub.status.busy":"2023-08-20T15:04:26.173203Z","iopub.execute_input":"2023-08-20T15:04:26.173612Z","iopub.status.idle":"2023-08-20T15:04:26.191847Z","shell.execute_reply.started":"2023-08-20T15:04:26.173576Z","shell.execute_reply":"2023-08-20T15:04:26.190789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Number of the same amnoacid sequence and all the same go-terms\ndf_duplicated_terms['terms_collected'] = df_duplicated_terms['terms_collected'].apply(tuple)\ndf_dp_count = df_duplicated_terms[df_duplicated_terms[['terms_collected','duplicated_sequence_group']].duplicated(keep=False)] #Count the number of proteins with same sequence and same go-terms","metadata":{"execution":{"iopub.status.busy":"2023-08-20T15:04:26.193825Z","iopub.execute_input":"2023-08-20T15:04:26.19443Z","iopub.status.idle":"2023-08-20T15:04:26.229653Z","shell.execute_reply.started":"2023-08-20T15:04:26.19439Z","shell.execute_reply":"2023-08-20T15:04:26.228713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Different proteins that have different functions although having the same sequence\ndf_dp_count","metadata":{"execution":{"iopub.status.busy":"2023-08-20T15:04:26.230846Z","iopub.execute_input":"2023-08-20T15:04:26.231268Z","iopub.status.idle":"2023-08-20T15:04:26.245627Z","shell.execute_reply.started":"2023-08-20T15:04:26.231233Z","shell.execute_reply":"2023-08-20T15:04:26.244514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Distribution betwen aspects\npie_df = train_terms['aspect'].value_counts()\npalette_color = sns.color_palette('bright')\nplt.pie(pie_df.values, labels=np.array(pie_df.index), colors=palette_color, autopct='%.0f%%')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-20T15:04:26.247115Z","iopub.execute_input":"2023-08-20T15:04:26.247474Z","iopub.status.idle":"2023-08-20T15:04:27.157458Z","shell.execute_reply.started":"2023-08-20T15:04:26.247443Z","shell.execute_reply":"2023-08-20T15:04:27.156265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from Bio import SeqIO\n\n# Specify the path to your FASTA file\nfasta_file = '/kaggle/input/cafa-5-protein-function-prediction/Train/train_sequences.fasta'\n\n# Read the FASTA file\nsequences = []\nfor record in SeqIO.parse(fasta_file, \"fasta\"):\n    # Access the sequence ID and sequence data\n    sequence_id = record.id\n    sequence_data = record.seq\n\n    # Add the sequence to the list\n    sequences.append((sequence_id, sequence_data))","metadata":{"execution":{"iopub.status.busy":"2023-08-20T15:04:27.158996Z","iopub.execute_input":"2023-08-20T15:04:27.159382Z","iopub.status.idle":"2023-08-20T15:04:30.244389Z","shell.execute_reply.started":"2023-08-20T15:04:27.159349Z","shell.execute_reply":"2023-08-20T15:04:30.243312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(sequences), sequences[0],sequences[0][1]","metadata":{"execution":{"iopub.status.busy":"2023-08-20T15:04:30.246096Z","iopub.execute_input":"2023-08-20T15:04:30.246715Z","iopub.status.idle":"2023-08-20T15:04:30.254159Z","shell.execute_reply.started":"2023-08-20T15:04:30.246681Z","shell.execute_reply":"2023-08-20T15:04:30.253094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preparing Data","metadata":{}},{"cell_type":"code","source":"# Set the limit for label\nnum_of_labels = 1500\ntrain_size = train_id.shape[0] # len(X)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T15:04:30.255702Z","iopub.execute_input":"2023-08-20T15:04:30.256047Z","iopub.status.idle":"2023-08-20T15:04:30.26409Z","shell.execute_reply.started":"2023-08-20T15:04:30.256014Z","shell.execute_reply":"2023-08-20T15:04:30.263126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.utils import pad_sequences\n\n# Define the dictionary mapping for tokenization\namino_acid_dict = {\n    'A': 1, 'R': 2, 'N': 3, 'D': 4, 'C': 5, 'Q': 6, 'E': 7, 'G': 8,\n    'H': 9, 'I': 10, 'L': 11, 'K': 12, 'M': 13, 'F': 14, 'P': 15, 'S': 16,\n    'T': 17, 'W': 18, 'Y': 19, 'V': 20, 'B': 21, 'Z': 22, 'X': 23, 'U': 24,\n    'O': 25\n}\namnoacid_sequences = []\n\n# Define the maximum sequence length\nmax_sequence_length = 400\n\n# Loop through each label\nfor sequence in sequences:\n#     print(sequence[1])\n    # Convert sequences to integer tokens\n    tokenized_seq = [amino_acid_dict[aa] for aa in sequence[1]]\n    \n    amnoacid_sequences.append(tokenized_seq)\n    \n# Pad or truncate sequences to the desired length\namnoacid_sequences = pad_sequences(amnoacid_sequences, maxlen=max_sequence_length, padding='post', truncating='post', value=0)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T15:04:30.265626Z","iopub.execute_input":"2023-08-20T15:04:30.265962Z","iopub.status.idle":"2023-08-20T15:04:41.623315Z","shell.execute_reply.started":"2023-08-20T15:04:30.265931Z","shell.execute_reply":"2023-08-20T15:04:41.622296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"amnoacid_sequences.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-20T15:04:41.624956Z","iopub.execute_input":"2023-08-20T15:04:41.625374Z","iopub.status.idle":"2023-08-20T15:04:41.632405Z","shell.execute_reply.started":"2023-08-20T15:04:41.625338Z","shell.execute_reply":"2023-08-20T15:04:41.631388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Take value counts in descending order and fetch first 1500 `GO term ID` as labels\nlabels = train_terms['term'].value_counts().index[:num_of_labels].tolist()\n\n# Fetch the train_terms data for the relevant labels only\ntrain_terms_updated = train_terms.loc[train_terms['term'].isin(labels)]\n\n# Setup progressbar settings.\nbar = progressbar.ProgressBar(maxval=num_of_labels, \\\n    widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n\n# Create an empty dataframe of required size for storing the labels,\ntrain_labels = np.zeros((train_size ,num_of_labels))\nseries_train_protein_ids = pd.Series(train_id)\n\n# Loop through each label\nfor i in range(num_of_labels):\n    # For each label, fetch the corresponding train_terms data\n    n_train_terms = train_terms_updated[train_terms_updated['term'] ==  labels[i]]\n    \n    # Fetch all the unique EntryId aka proteins related to the current label(GO term ID)\n    label_related_proteins = n_train_terms['EntryID'].unique()\n    \n    # In the series_train_protein_ids pandas series, if a protein is related\n    # to the current label, then mark it as 1, else 0.\n    # Replace the ith column of train_Y with with that pandas series.\n    train_labels[:,i] =  series_train_protein_ids.isin(label_related_proteins).astype(float)\n    \n    # Progress bar percentage increase\n    bar.update(i+1)\n\n# Notify the end of progress bar \nbar.finish()\n\n# Convert train_Y numpy into pandas dataframe\nlabels_df = pd.DataFrame(data = train_labels, columns = labels)\nprint(labels_df.shape)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T15:04:41.634101Z","iopub.execute_input":"2023-08-20T15:04:41.634719Z","iopub.status.idle":"2023-08-20T15:23:00.455184Z","shell.execute_reply.started":"2023-08-20T15:04:41.634681Z","shell.execute_reply":"2023-08-20T15:23:00.454184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract input features and labels from the DataFrame\nfeatures_input = train_df.loc[:, train_df.columns != 'ID'].values  # Extract the values from the DataFrame\nlabels_input = labels_df.values  # Extract the label column\n","metadata":{"execution":{"iopub.status.busy":"2023-08-20T15:23:00.45662Z","iopub.execute_input":"2023-08-20T15:23:00.456954Z","iopub.status.idle":"2023-08-20T15:23:00.882252Z","shell.execute_reply.started":"2023-08-20T15:23:00.456923Z","shell.execute_reply":"2023-08-20T15:23:00.881133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_input","metadata":{"execution":{"iopub.status.busy":"2023-08-20T15:23:00.883843Z","iopub.execute_input":"2023-08-20T15:23:00.885633Z","iopub.status.idle":"2023-08-20T15:23:00.897487Z","shell.execute_reply.started":"2023-08-20T15:23:00.885596Z","shell.execute_reply":"2023-08-20T15:23:00.896471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Eval in test data \ntest_embeddings = np.load('/kaggle/input/t5embeds/test_embeds.npy')\n\n# Convert test_embeddings to dataframe\ncolumn_num = test_embeddings.shape[1]\ntest_df = pd.DataFrame(test_embeddings, columns = [\"Column_\" + str(i) for i in range(1, column_num+1)])\nprint(test_df.shape)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T15:23:00.899903Z","iopub.execute_input":"2023-08-20T15:23:00.901313Z","iopub.status.idle":"2023-08-20T15:23:13.034883Z","shell.execute_reply.started":"2023-08-20T15:23:00.901263Z","shell.execute_reply":"2023-08-20T15:23:13.033835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tryng a CNN-LSTM","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Conv1D, MaxPooling1D, LSTM, Dense\nimport tensorflow as tf\n\n# Use MirroredStrategy for multi-GPU training\nstrategy = tf.distribute.MirroredStrategy()\nwith strategy.scope():\n    # Create a sequential model\n    model_CNN_LSTM = Sequential()\n    \n    # Add a Conv1D layer for spatial pattern detection\n    model_CNN_LSTM.add(Conv1D(32, kernel_size=3, input_shape=(1024, 1), activation='relu'))\n    model_CNN_LSTM.add(MaxPooling1D(pool_size=2))\n    \n    # Add an LSTM layer for sequence modeling\n    model_CNN_LSTM.add(LSTM(units=64, dropout=0.2, recurrent_dropout=0.2))\n    \n    # Add another Dense layer for non-linear transformations\n    model_CNN_LSTM.add(Dense(128, activation='relu'))\n\n    # Add a fully connected layer for classification\n    model_CNN_LSTM.add(Dense(1500, activation='softmax'))\n    \n    # Compile the model\n    model_CNN_LSTM.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Define Early Stopping callback\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',  # Monitor validation loss\n    patience=5,           # Number of epochs with no improvement after which training will be stopped\n    restore_best_weights=True  # Restore weights from the epoch with the best value of the monitored quantity\n)\n\n# Assuming you have training and validation data\n# model_CNN_LSTM.fit(train_data, train_labels, validation_data=(val_data, val_labels), callbacks=[early_stopping], epochs=50)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-20T16:25:23.801471Z","iopub.execute_input":"2023-08-20T16:25:23.801853Z","iopub.status.idle":"2023-08-20T16:25:24.605923Z","shell.execute_reply.started":"2023-08-20T16:25:23.801825Z","shell.execute_reply":"2023-08-20T16:25:24.605183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\n# Check if GPU devices are available\nphysical_devices = tf.config.list_physical_devices('GPU')\nnum_gpus = len(physical_devices)\nprint(\"Number of available GPUs:\", num_gpus)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-20T15:23:16.049851Z","iopub.execute_input":"2023-08-20T15:23:16.050214Z","iopub.status.idle":"2023-08-20T15:23:16.056665Z","shell.execute_reply.started":"2023-08-20T15:23:16.050181Z","shell.execute_reply":"2023-08-20T15:23:16.055954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if num_gpus < 2:\n    print(\"Not enough GPUs available. Training on a single GPU.\")\n    history_CNN = model_CNN_LSTM.fit(features_input, labels_input, epochs=10, batch_size=1024)# Train the model with GPU acceleration\nelse:\n    #Use MirroredStrategy for multi-GPU training\n    with strategy.scope():\n        history_CNN = model_CNN_LSTM.fit(features_input, labels_input, epochs=10, batch_size=1024)# Train the model with GPU acceleration","metadata":{"execution":{"iopub.status.busy":"2023-08-20T16:25:30.608769Z","iopub.execute_input":"2023-08-20T16:25:30.60918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_df = pd.DataFrame(history_CNN.history)\nplt.subplots_adjust(wspace = 0.3, hspace = 0.3)\nplt.figure(figsize=(10,10))\n\nplt.subplot(2,2,1)\nplt.plot(history_CNN.history['loss'])\nplt.xlabel('epoch')\nplt.ylabel('loss')\n\nplt.subplot(2,2,2)\nplt.plot(history_CNN.history['accuracy'])\nplt.xlabel('epoch')\nplt.ylabel('accuracy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import save_model\nmodel_CNN_LSTM.save(\"model_CNN_LSTM_ReLU_softmax.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nmodel_CNN_LSTM = load_model(\"/kaggle/input/cafa5-test/model_CNN_LSTM_softmax.h5\")","metadata":{"execution":{"iopub.status.busy":"2023-08-20T16:16:49.018726Z","iopub.execute_input":"2023-08-20T16:16:49.019427Z","iopub.status.idle":"2023-08-20T16:16:49.37209Z","shell.execute_reply.started":"2023-08-20T16:16:49.019395Z","shell.execute_reply":"2023-08-20T16:16:49.371076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\ntest_embeddings = np.load('/kaggle/input/t5embeds/test_embeds.npy')\n\n# Convert test_embeddings to dataframe\ncolumn_num = test_embeddings.shape[1]\ntest_df = pd.DataFrame(test_embeddings, columns = [\"Column_\" + str(i) for i in range(1, column_num+1)])\nprint(test_df.shape)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T16:16:49.377518Z","iopub.execute_input":"2023-08-20T16:16:49.378096Z","iopub.status.idle":"2023-08-20T16:17:04.10653Z","shell.execute_reply.started":"2023-08-20T16:16:49.378061Z","shell.execute_reply":"2023-08-20T16:17:04.104157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions =  model_CNN_LSTM.predict(test_df)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T16:17:04.107991Z","iopub.execute_input":"2023-08-20T16:17:04.109036Z","iopub.status.idle":"2023-08-20T16:17:52.38365Z","shell.execute_reply.started":"2023-08-20T16:17:04.109Z","shell.execute_reply":"2023-08-20T16:17:52.382206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-08-20T16:17:52.384945Z","iopub.status.idle":"2023-08-20T16:17:52.385662Z","shell.execute_reply.started":"2023-08-20T16:17:52.385415Z","shell.execute_reply":"2023-08-20T16:17:52.385439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions","metadata":{"execution":{"iopub.status.busy":"2023-08-20T16:17:52.387028Z","iopub.status.idle":"2023-08-20T16:17:52.387833Z","shell.execute_reply.started":"2023-08-20T16:17:52.387546Z","shell.execute_reply":"2023-08-20T16:17:52.387571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.save(\"predictions.npy\", predictions)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T16:17:52.389277Z","iopub.status.idle":"2023-08-20T16:17:52.389971Z","shell.execute_reply.started":"2023-08-20T16:17:52.389715Z","shell.execute_reply":"2023-08-20T16:17:52.389737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n\n# predictions = np.load(\"/kaggle/input/cafa5-test/predictions.npy\")\n# test_protein_ids = np.load('/kaggle/input/t5embeds/test_ids.npy')\n# train_terms = pd.read_csv('/kaggle/input/cafa-5-protein-function-prediction/Train/train_terms.tsv',sep='\\t')\n\n# # Take value counts in descending order and fetch first 1500 `GO term ID` as labels\n# num_of_labels = 1500\n# labels = train_terms['term'].value_counts().index[:num_of_labels].tolist()\n\n# df_submission = pd.DataFrame(columns = ['Protein Id', 'GO Term Id','Prediction'])\n\n# l = []\n# for k in list(test_protein_ids):\n#     l += [ k] * predictions.shape[1]\n    \n# df_submission['Protein Id'] = l\n# df_submission['GO Term Id'] = labels * predictions.shape[0]\n# df_submission['Prediction'] = predictions.ravel()\n# df_submission.to_csv(\"submission.tsv\",header=False, index=False, sep=\"\\t\")","metadata":{"execution":{"iopub.status.busy":"2023-08-20T16:17:52.391263Z","iopub.status.idle":"2023-08-20T16:17:52.391958Z","shell.execute_reply.started":"2023-08-20T16:17:52.391703Z","shell.execute_reply":"2023-08-20T16:17:52.391726Z"},"trusted":true},"execution_count":null,"outputs":[]}]}